{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzmGfIUSEnQqi8lLPNIOdM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryandale7/ML-on-Graphs/blob/main/Machine_Learning_on_Graphs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What is a Graph Neural Network?**\n",
        "\n",
        "**GNNs** are a type of neural network specifically designed to work with **graph-structured data**. Unlike traditional neural networks, which process structured inputs like sequences or grids (e.g., text or images), GNNs can handle **nodes** (entities) and **edges** (relationships) in graphs, making them suitable for problems where data is best represented as a graph.\n",
        "\n",
        "\n",
        "A graph \\( G \\) is a mathematical structure represented as:  \n",
        "\\[\n",
        "G = (V, E)\n",
        "\\]  \n",
        "where:\n",
        "- \\( V \\): Set of **nodes** (or vertices), representing entities.\n",
        "- \\( E \\): Set of **edges**, representing relationships between entities.\n",
        "- \\( A \\): **Adjacency matrix** describing connections between nodes. \\( A_{ij} = 1 \\) if an edge exists between nodes \\( i \\) and \\( j \\).\n",
        "\n",
        "Each node \\( v \\in V \\) or edge \\( e_{ij} \\in E \\) can also carry **features**. For example:\n",
        "- In a **social network**, nodes are people, edges are friendships, and node features could include user attributes (age, interests, etc.).\n",
        "- In a **molecular graph**, nodes are atoms, edges are chemical bonds, and node features represent atom types."
      ],
      "metadata": {
        "id": "LyZNB2xpKZvz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1: Dependencies**\n",
        "We will only use `numpy` for numerical operations. Install it if necessary:"
      ],
      "metadata": {
        "id": "lLxhEUUhL87s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgX0YkOGL9YF",
        "outputId": "c2c11848-36de-43e1-83e0-a12138390bc3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2: Define the Graph Structure**\n",
        "We represent the graph as an adjacency matrix \\( A \\), node features \\( X \\), and labels \\( Y \\).\n",
        "\n"
      ],
      "metadata": {
        "id": "WwG1eiM8MZPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Adjacency Matrix (undirected graph with 4 nodes)\n",
        "A = np.array([\n",
        "    [0, 1, 0, 0],\n",
        "    [1, 0, 1, 0],\n",
        "    [0, 1, 0, 1],\n",
        "    [0, 0, 1, 0]\n",
        "])\n",
        "\n",
        "# Node features (4 nodes, 3 features each)\n",
        "X = np.array([\n",
        "    [1, 2, 1],\n",
        "    [2, 1, 0],\n",
        "    [3, 1, 4],\n",
        "    [4, 2, 1]\n",
        "])\n",
        "\n",
        "# Node labels (for classification, e.g., binary)\n",
        "Y = np.array([0, 1, 0, 1])  # Ground truth labels\n"
      ],
      "metadata": {
        "id": "Rz0prbjLMjf8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Why Do We Need GNNs?**\n",
        "\n",
        "Graphs are everywhere in the real world:\n",
        "- **Social Networks**: Users (nodes) and friendships (edges).\n",
        "- **Molecules**: Atoms (nodes) and bonds (edges).\n",
        "- **Knowledge Graphs**: Entities (nodes) and relationships (edges).\n",
        "- **Transportation Networks**: Locations (nodes) and routes (edges).\n",
        "- **Recommender Systems**: Users and items as nodes, interactions as edges.\n",
        "\n",
        "Traditional machine learning methods struggle to capture graph structure because:\n",
        "1. Graphs are non-Euclidean, meaning they lack a regular grid-like structure.\n",
        "2. Nodes and edges have varying numbers of connections.\n",
        "\n",
        "GNNs overcome this by **propagating and learning information** through graph structures."
      ],
      "metadata": {
        "id": "XT7vrzXSKZek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 3: Implement GNN Layers**\n",
        "We perform **message passing** and a simple forward pass using matrix multiplication.\n",
        "\n",
        "1. Add self-loops to the adjacency matrix.\n",
        "2. Normalize the adjacency matrix.\n",
        "3. Apply a linear transformation and activation function."
      ],
      "metadata": {
        "id": "2TwEbcMKUz-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Add self-loops to the adjacency matrix\n",
        "I = np.eye(A.shape[0])  # Identity matrix\n",
        "A_hat = A + I  # Add self-loops\n",
        "\n",
        "# Step 2: Normalize adjacency matrix\n",
        "D = np.diag(np.sum(A_hat, axis=1))  # Degree matrix\n",
        "D_inv_sqrt = np.linalg.inv(np.sqrt(D))  # D^(-1/2)\n",
        "A_norm = D_inv_sqrt @ A_hat @ D_inv_sqrt  # Normalized adjacency matrix\n",
        "\n",
        "# Step 3: Define a GNN Layer\n",
        "def gnn_layer(X, A_norm, W):\n",
        "    \"\"\"\n",
        "    Single GNN layer.\n",
        "    Args:\n",
        "        X: Input node features [N, F_in]\n",
        "        A_norm: Normalized adjacency matrix [N, N]\n",
        "        W: Weight matrix [F_in, F_out]\n",
        "    Returns:\n",
        "        Updated node features [N, F_out]\n",
        "    \"\"\"\n",
        "    return np.maximum(0, A_norm @ X @ W)  # ReLU activation\n"
      ],
      "metadata": {
        "id": "Yh3eeGZdUzdk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 4: Build the GNN Model**\n",
        "We use two GNN layers with ReLU activation to learn node embeddings and make predictions.\n"
      ],
      "metadata": {
        "id": "iOa8IA8HVOY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights for two GNN layers\n",
        "np.random.seed(42)\n",
        "W1 = np.random.randn(3, 4)  # Layer 1: input 3 -> output 4\n",
        "W2 = np.random.randn(4, 2)  # Layer 2: input 4 -> output 2 (binary classification)\n",
        "\n",
        "# Forward pass\n",
        "H1 = gnn_layer(X, A_norm, W1)  # First GNN layer\n",
        "H2 = gnn_layer(H1, A_norm, W2)  # Second GNN layer\n",
        "\n",
        "# Softmax for predictions\n",
        "def softmax(logits):\n",
        "    exp_logits = np.exp(logits)\n",
        "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
        "\n",
        "predictions = softmax(H2)\n",
        "print(\"Node Predictions (Probabilities):\\n\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DISH36TRVST1",
        "outputId": "1a26da5b-5afd-4f04-9909-4aafd4e432a3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node Predictions (Probabilities):\n",
            " [[0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 5: Loss Function and Training**\n",
        "To train the GNN, implement a simple **cross-entropy loss** and a training loop using gradient descent."
      ],
      "metadata": {
        "id": "aV0YyvGDVUJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-entropy loss\n",
        "def cross_entropy(pred, labels):\n",
        "    n = labels.shape[0]\n",
        "    log_probs = -np.log(pred[range(n), labels])\n",
        "    return np.sum(log_probs) / n\n",
        "\n",
        "# Gradient descent\n",
        "learning_rate = 0.01\n",
        "for epoch in range(100):\n",
        "    # Forward pass\n",
        "    H1 = gnn_layer(X, A_norm, W1)\n",
        "    H2 = gnn_layer(H1, A_norm, W2)\n",
        "    pred = softmax(H2)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = cross_entropy(pred, Y)\n",
        "\n",
        "    # Backpropagation (manual gradients, simple approximation)\n",
        "    grad_H2 = pred\n",
        "    grad_H2[range(len(Y)), Y] -= 1\n",
        "    grad_H2 /= len(Y)\n",
        "\n",
        "    grad_W2 = H1.T @ (A_norm @ grad_H2)\n",
        "    grad_H1 = A_norm @ (grad_H2 @ W2.T) * (H1 > 0)  # Backprop through ReLU\n",
        "    grad_W1 = X.T @ (A_norm @ grad_H1)\n",
        "\n",
        "    # Update weights\n",
        "    W2 -= learning_rate * grad_W2\n",
        "    W1 -= learning_rate * grad_W1\n",
        "\n",
        "    # Print loss\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctrVTBVvVY-I",
        "outputId": "d38cb126-3ddf-4bb8-f408-38a4c664e260"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.6931\n",
            "Epoch 10, Loss: 0.6931\n",
            "Epoch 20, Loss: 0.6931\n",
            "Epoch 30, Loss: 0.6931\n",
            "Epoch 40, Loss: 0.6931\n",
            "Epoch 50, Loss: 0.6931\n",
            "Epoch 60, Loss: 0.6931\n",
            "Epoch 70, Loss: 0.6931\n",
            "Epoch 80, Loss: 0.6931\n",
            "Epoch 90, Loss: 0.6931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 6: Evaluate the Model**\n",
        "\n",
        "After training, check the predictions for each node:"
      ],
      "metadata": {
        "id": "wx5JLFOPVdA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final predictions\n",
        "H1 = gnn_layer(X, A_norm, W1)\n",
        "H2 = gnn_layer(H1, A_norm, W2)\n",
        "pred = softmax(H2)\n",
        "predicted_labels = np.argmax(pred, axis=1)\n",
        "\n",
        "print(\"True Labels:      \", Y)\n",
        "print(\"Predicted Labels: \", predicted_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZiony1OVhlw",
        "outputId": "827624d2-091d-4341-b2bb-025e458ec4a1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True Labels:       [0 1 0 1]\n",
            "Predicted Labels:  [0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **How Do GNNs Work?**\n",
        "\n",
        "GNNs use a **message-passing mechanism** to allow nodes to exchange information with their neighbors iteratively.\n",
        "\n",
        "1. **Node Representation Initialization**:  \n",
        "   Each node starts with initial features \\( h_v^{(0)} \\).\n",
        "\n",
        "2. **Message Passing**:  \n",
        "   At each iteration (or \"layer\"), a node aggregates information from its neighbors. This can be written as:\n",
        "   \\[\n",
        "   h_v^{(t+1)} = \\text{UPDATE}\\left(h_v^{(t)}, \\text{AGGREGATE}\\left(\\{h_u^{(t)} : u \\in \\mathcal{N}(v)\\}\\right)\\right)\n",
        "   \\]\n",
        "   - **AGGREGATE**: Combines information from the neighbors of \\( v \\) (e.g., using sum, mean, max, or attention mechanisms).\n",
        "   - **UPDATE**: Updates the node's representation using the aggregated information (e.g., with a neural network).\n",
        "\n",
        "3. **Final Representation**:  \n",
        "   After \\( T \\) layers of message passing, nodes have learned **contextual embeddings** that encode both their features and the graph's structure.\n",
        "\n",
        "4. **Graph-Level Representations**:  \n",
        "   To produce a representation for the whole graph, a **pooling** operation (e.g., sum, mean, or max) combines all node representations."
      ],
      "metadata": {
        "id": "KEcxf02RKZTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Types of GNNs**\n",
        "\n",
        "### 1. **Graph Convolutional Networks (GCN)**\n",
        "- Generalizes traditional convolutions to graphs.\n",
        "- Uses a normalized adjacency matrix to propagate information between nodes.\n",
        "\n",
        "### 2. **Graph Attention Networks (GAT)**\n",
        "- Uses **attention mechanisms** to assign different weights to neighboring nodes during aggregation.\n",
        "\n",
        "### 3. **GraphSAGE**\n",
        "- Scalable GNN for large graphs.\n",
        "- Samples neighborhoods and aggregates information using mean, LSTM, or pooling.\n",
        "\n",
        "### 4. **Message Passing Neural Networks (MPNNs)**\n",
        "- A general framework where messages are passed between nodes through edges.\n",
        "\n",
        "### 5. **Graph Isomorphism Networks (GIN)**\n",
        "- A powerful model capable of distinguishing graph structures effectively."
      ],
      "metadata": {
        "id": "4FZm3P2RKZHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 7: Define a Simple GCN Layer**\n",
        "\n",
        "Each GCN layer aggregates information from neighboring nodes.\n",
        "\n",
        "\\[\n",
        "H = \\text{ReLU}(A_{\\text{norm}} \\cdot X \\cdot W)\n",
        "\\]\n",
        "\n",
        "- \\( A_{\\text{norm}} \\): Normalized adjacency matrix.\n",
        "- \\( X \\): Node features.\n",
        "- \\( W \\): Weight matrix.\n",
        "- ReLU activation: \\( \\max(0, x) \\)."
      ],
      "metadata": {
        "id": "5DniupHuV-9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gcn_layer(X, A_norm, W):\n",
        "    \"\"\"\n",
        "    Single GCN layer: Aggregates node information.\n",
        "    Args:\n",
        "        X: Node features [N, F_in]\n",
        "        A_norm: Normalized adjacency matrix [N, N]\n",
        "        W: Weight matrix [F_in, F_out]\n",
        "    Returns:\n",
        "        Updated node features [N, F_out]\n",
        "    \"\"\"\n",
        "    return np.maximum(0, A_norm @ X @ W)  # ReLU activation\n"
      ],
      "metadata": {
        "id": "js6LLmm5V9n-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 8: Build the GCN Model**\n",
        "\n",
        "We define a 2-layer GCN for node classification."
      ],
      "metadata": {
        "id": "5tkgCByPWUUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weight matrices\n",
        "np.random.seed(42)\n",
        "W1 = np.random.randn(3, 4)  # Layer 1: input 3 -> output 4\n",
        "W2 = np.random.randn(4, 2)  # Layer 2: input 4 -> output 2 (binary classification)\n",
        "\n",
        "# Forward pass through GCN\n",
        "H1 = gcn_layer(X, A_norm, W1)  # First GCN layer\n",
        "H2 = gcn_layer(H1, A_norm, W2)  # Second GCN layer\n",
        "\n",
        "# Softmax activation for final predictions\n",
        "def softmax(logits):\n",
        "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
        "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
        "\n",
        "predictions = softmax(H2)\n",
        "print(\"Predictions (Node Probabilities):\\n\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kntz00iiWVce",
        "outputId": "19cc64f0-1666-4adf-e262-9e41c7d45e0d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions (Node Probabilities):\n",
            " [[0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 9: Cross-Entropy Loss**\n",
        "\n",
        "We compute the cross-entropy loss to measure the difference between predictions and labels.\n"
      ],
      "metadata": {
        "id": "j5q7xDHIWkuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(pred, labels):\n",
        "    \"\"\"\n",
        "    Compute cross-entropy loss.\n",
        "    Args:\n",
        "        pred: Predicted probabilities [N, C]\n",
        "        labels: Ground truth labels [N]\n",
        "    \"\"\"\n",
        "    n = labels.shape[0]\n",
        "    log_probs = -np.log(pred[range(n), labels])\n",
        "    return np.sum(log_probs) / n\n",
        "\n",
        "# Compute loss\n",
        "loss = cross_entropy_loss(predictions, Y)\n",
        "print(\"Cross-Entropy Loss:\", loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNe13OXWWoaS",
        "outputId": "def035cb-cc6f-471e-e072-012045842b9a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Entropy Loss: 0.6931471805599453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 10: Training the GCN**\n",
        "\n",
        "We train the GCN using gradient descent to minimize the loss."
      ],
      "metadata": {
        "id": "2YTmCYnwWuMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "learning_rate = 0.01\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    H1 = gcn_layer(X, A_norm, W1)\n",
        "    H2 = gcn_layer(H1, A_norm, W2)\n",
        "    pred = softmax(H2)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = cross_entropy_loss(pred, Y)\n",
        "\n",
        "    # Compute gradients (approximation for simplicity)\n",
        "    grad_H2 = pred\n",
        "    grad_H2[range(len(Y)), Y] -= 1\n",
        "    grad_H2 /= len(Y)\n",
        "\n",
        "    grad_W2 = H1.T @ (A_norm @ grad_H2)\n",
        "    grad_H1 = A_norm @ (grad_H2 @ W2.T) * (H1 > 0)  # Backpropagate through ReLU\n",
        "    grad_W1 = X.T @ (A_norm @ grad_H1)\n",
        "\n",
        "    # Update weights\n",
        "    W2 -= learning_rate * grad_W2\n",
        "    W1 -= learning_rate * grad_W1\n",
        "\n",
        "    # Print loss every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1TBnNwAWwZd",
        "outputId": "41f5e86c-94fc-4045-9191-c218adc6664a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.6931\n",
            "Epoch 10, Loss: 0.6931\n",
            "Epoch 20, Loss: 0.6931\n",
            "Epoch 30, Loss: 0.6931\n",
            "Epoch 40, Loss: 0.6931\n",
            "Epoch 50, Loss: 0.6931\n",
            "Epoch 60, Loss: 0.6931\n",
            "Epoch 70, Loss: 0.6931\n",
            "Epoch 80, Loss: 0.6931\n",
            "Epoch 90, Loss: 0.6931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 11: Evaluate the Model**\n",
        "\n",
        "After training, check the final predictions and compare them with true labels."
      ],
      "metadata": {
        "id": "F129iVjSW83Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final predictions\n",
        "H1 = gcn_layer(X, A_norm, W1)\n",
        "H2 = gcn_layer(H1, A_norm, W2)\n",
        "pred = softmax(H2)\n",
        "predicted_labels = np.argmax(pred, axis=1)\n",
        "\n",
        "print(\"True Labels:      \", Y)\n",
        "print(\"Predicted Labels: \", predicted_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWw0l9FLW95R",
        "outputId": "b37a9c38-544c-4fc7-9ab3-76aa3da44a03"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True Labels:       [0 1 0 1]\n",
            "Predicted Labels:  [0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Applications of GNNs**\n",
        "\n",
        "1. **Node Classification**: Predict node labels (e.g., categorizing users in social networks).\n",
        "2. **Link Prediction**: Predict edges between nodes (e.g., friend recommendations).\n",
        "3. **Graph Classification**: Classify entire graphs (e.g., molecular property prediction).\n",
        "4. **Community Detection**: Identify groups of closely connected nodes.\n",
        "5. **Recommender Systems**: Use user-item graphs to predict preferences.\n",
        "6. **Knowledge Graph Completion**: Predict missing relationships between entities.\n",
        "7. **Traffic Networks**: Analyze and predict traffic patterns in road networks."
      ],
      "metadata": {
        "id": "T-n1dtDaKY8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Advantages of GNNs**\n",
        "\n",
        "- GNNs can model both **node features** and **graph structures**.\n",
        "- They work on irregular, non-Euclidean data like graphs.\n",
        "- They allow nodes to learn from their **local neighborhoods**."
      ],
      "metadata": {
        "id": "EMGd2e4jKYxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Popular Frameworks for GNNs**\n",
        "\n",
        "- **PyTorch Geometric**: GNN library built on PyTorch.\n",
        "- **Deep Graph Library (DGL)**: Flexible and scalable framework.\n",
        "- **Spektral**: GNN library for TensorFlow/Keras.\n",
        "- **NetworkX**: Graph processing library (non-deep learning)."
      ],
      "metadata": {
        "id": "K3uTggIqKYj-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5sCG9e_JRBd",
        "outputId": "ee4f6530-a724-45f1-f271-0ae78bdbc882"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "pip install numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Adjacency Matrix (undirected graph with 4 nodes)\n",
        "A = np.array([\n",
        "    [0, 1, 0, 0],\n",
        "    [1, 0, 1, 0],\n",
        "    [0, 1, 0, 1],\n",
        "    [0, 0, 1, 0]\n",
        "])\n",
        "\n",
        "# Node features (4 nodes, 3 features each)\n",
        "X = np.array([\n",
        "    [1, 2, 1],\n",
        "    [2, 1, 0],\n",
        "    [3, 1, 4],\n",
        "    [4, 2, 1]\n",
        "])\n",
        "\n",
        "# Node labels (for classification, e.g., binary)\n",
        "Y = np.array([0, 1, 0, 1])  # Ground truth labels\n"
      ],
      "metadata": {
        "id": "DtJFDOQzJbAJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Add self-loops to the adjacency matrix\n",
        "I = np.eye(A.shape[0])  # Identity matrix\n",
        "A_hat = A + I  # Add self-loops\n",
        "\n",
        "# Step 2: Normalize adjacency matrix\n",
        "D = np.diag(np.sum(A_hat, axis=1))  # Degree matrix\n",
        "D_inv_sqrt = np.linalg.inv(np.sqrt(D))  # D^(-1/2)\n",
        "A_norm = D_inv_sqrt @ A_hat @ D_inv_sqrt  # Normalized adjacency matrix\n",
        "\n",
        "# Step 3: Define a GNN Layer\n",
        "def gnn_layer(X, A_norm, W):\n",
        "    \"\"\"\n",
        "    Single GNN layer.\n",
        "    Args:\n",
        "        X: Input node features [N, F_in]\n",
        "        A_norm: Normalized adjacency matrix [N, N]\n",
        "        W: Weight matrix [F_in, F_out]\n",
        "    Returns:\n",
        "        Updated node features [N, F_out]\n",
        "    \"\"\"\n",
        "    return np.maximum(0, A_norm @ X @ W)  # ReLU activation\n"
      ],
      "metadata": {
        "id": "snqlH-wuJksL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights for two GNN layers\n",
        "np.random.seed(42)\n",
        "W1 = np.random.randn(3, 4)  # Layer 1: input 3 -> output 4\n",
        "W2 = np.random.randn(4, 2)  # Layer 2: input 4 -> output 2 (binary classification)\n",
        "\n",
        "# Forward pass\n",
        "H1 = gnn_layer(X, A_norm, W1)  # First GNN layer\n",
        "H2 = gnn_layer(H1, A_norm, W2)  # Second GNN layer\n",
        "\n",
        "# Softmax for predictions\n",
        "def softmax(logits):\n",
        "    exp_logits = np.exp(logits)\n",
        "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
        "\n",
        "predictions = softmax(H2)\n",
        "print(\"Node Predictions (Probabilities):\\n\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJgIa1b0Jt13",
        "outputId": "e850cb10-6cec-43fb-d163-f70f762875f7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node Predictions (Probabilities):\n",
            " [[0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-entropy loss\n",
        "def cross_entropy(pred, labels):\n",
        "    n = labels.shape[0]\n",
        "    log_probs = -np.log(pred[range(n), labels])\n",
        "    return np.sum(log_probs) / n\n",
        "\n",
        "# Gradient descent\n",
        "learning_rate = 0.01\n",
        "for epoch in range(100):\n",
        "    # Forward pass\n",
        "    H1 = gnn_layer(X, A_norm, W1)\n",
        "    H2 = gnn_layer(H1, A_norm, W2)\n",
        "    pred = softmax(H2)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = cross_entropy(pred, Y)\n",
        "\n",
        "    # Backpropagation (manual gradients, simple approximation)\n",
        "    grad_H2 = pred\n",
        "    grad_H2[range(len(Y)), Y] -= 1\n",
        "    grad_H2 /= len(Y)\n",
        "\n",
        "    grad_W2 = H1.T @ (A_norm @ grad_H2)\n",
        "    grad_H1 = A_norm @ (grad_H2 @ W2.T) * (H1 > 0)  # Backprop through ReLU\n",
        "    grad_W1 = X.T @ (A_norm @ grad_H1)\n",
        "\n",
        "    # Update weights\n",
        "    W2 -= learning_rate * grad_W2\n",
        "    W1 -= learning_rate * grad_W1\n",
        "\n",
        "    # Print loss\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsPly19sJ0d0",
        "outputId": "a54367ea-76a7-4428-a922-944e80209955"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.6931\n",
            "Epoch 10, Loss: 0.6931\n",
            "Epoch 20, Loss: 0.6931\n",
            "Epoch 30, Loss: 0.6931\n",
            "Epoch 40, Loss: 0.6931\n",
            "Epoch 50, Loss: 0.6931\n",
            "Epoch 60, Loss: 0.6931\n",
            "Epoch 70, Loss: 0.6931\n",
            "Epoch 80, Loss: 0.6931\n",
            "Epoch 90, Loss: 0.6931\n"
          ]
        }
      ]
    }
  ]
}